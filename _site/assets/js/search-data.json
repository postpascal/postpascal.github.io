var store = [{
        "title": "MathJax test Fourier Transform",
        "excerpt":"Fourier Transform: Inverse Fourier Transform: ","categories": ["math"],
        "tags": ["Fourier Transform"],
        "url": "http://localhost:4000/math/test/"
      },{
        "title": "PATH",
        "excerpt":"    Table of Contents    How to add commands to $PATH in Linux or Mac?          What is PATH?                  Option1:Set PATH for your current shell session          Option2:Change your PATH permanently                    How to add commands to $PATH in Linux or Mac? What is PATH? PATH is a variable in $ \\star$nix system which tells the command path Option1:Set PATH for your current shell session export PATH=$PATH:/path/to/your/directory  DoneOption2:Change your PATH permanently   Edit your .bash_profile(if you ues Bash) or .zshrc( if using zsh)  add path at the end of the fileexport PATH=$PATH:/path/to/your/directoryORexport PATH=/path/to/your/directory:$PATH  Active your changessource .bash_profileORsource .zshrc  Done","categories": ["linux"],
        "tags": ["path"],
        "url": "http://localhost:4000/linux/path/"
      },{
        "title": "Maximum Likelihood Estimation",
        "excerpt":"    Table of Contents    Bayesian Rule:  Maximum Likelihood Estimation (MLE)Bayesian Rule: $P(D \\mid \\theta) \\Rightarrow$ Likelihood $P(\\theta) \\Rightarrow$ prior $P(\\theta \\mid D) \\Rightarrow$ posterior $P(D) \\Rightarrow$ marginal likelihood Maximum Likelihood Estimation (MLE) AKA:最大似然估计，likelihood AKA: 似然函数 Apply log operation to avoid underflow For instance,if  ","categories": ["math"],
        "tags": ["MLE"],
        "url": "http://localhost:4000/math/mle/"
      },{
        "title": "PCA",
        "excerpt":"    Table of Contents    方差  协方差  协方差矩阵  算法及实例          PCA算法      实例      进一步讨论      Blog Source: 方差 上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即： 由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示： 于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。 协方差 对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。 数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则： 可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。 当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。 至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 协方差矩阵 上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。 我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X： 然后我们用X乘以X的转置，并乘上系数$1/m$： 奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。 根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况： 设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。 ##协方差矩阵对角化根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系： 设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系： 现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。 至此，我们离“发明”PCA还有仅一步之遥！ 现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。 由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质： 1）实对称矩阵不同特征值对应的特征向量必然正交。 2）设特征向量重数为r，则必然存在r个线性无关的特征向量对应于\\lambda，因此可以将这r个特征向量单位正交化。 由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为，我们将其按列组成矩阵： 则对协方差矩阵C有如下结论： 其中为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。 以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。 到这里，我们发现我们已经找到了需要的矩阵P： P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。 至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。 算法及实例 为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。 PCA算法 总结一下PCA的算法步骤： 设有m条n维数据。 1）将原始数据按列组成n行m列矩阵X 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 3）求出协方差矩阵 4）求出协方差矩阵的特征值及对应的特征向量 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P 6）Y=PX即为降维到k维后的数据 实例 这里以上文提到的 为例，我们用PCA方法将这组二维数据其降到一维。 因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵： 然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为： 其对应的特征向量分别是： 其中对应的特征向量分别是一个通解，可取任意实数。那么标准化后的特征向量为： 因此我们的矩阵P是： 可以验证协方差矩阵C的对角化： 最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示： 降维投影结果如下图:     PCA image.进一步讨论 根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。 因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。 最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。 希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。 ","categories": ["machine learning"],
        "tags": ["math","PCA","dimension reduction"],
        "url": "http://localhost:4000/machine%20learning/pca/"
      },{
        "title": "智能时代",
        "excerpt":"智能时代 出版时间:2017作者:吴军正如书名一样，书里描绘里一个智能的时代。或者说是一个数据驱动的时代。科技的进步来源于人类对自然或或者说是事物的认知。也可以换一种说法，是来源于数据。人类的进步就是数据的积累。人们不断的从数据里总结，学习。从而才能产生知识—-一种对数据加工而得到的结论。人类的早期对数据的收集都比较慢，即没有工具，也没有处理的方法。这也就造成了人类的认知不足。随着时间的发展，数据量渐渐增大，人类可以从数据中提炼，挖掘出有效的信息，根据先验知识或者是由数据驱动。这就是我的工作，有效的挖掘数据的潜在价值，利用数据提取信息，进而用这些信息做一些事。所谓智能，不过是把数据中的信息挖掘出智能。比方说图像分类，人脸识别，声音识别。都是来源于数据，并且对数据加工，提取数据的共性，并且放大那些决定性的差异，然后才进行分类。另外一方面也十分重要，那就是对数据的收集。或者说是制造数据。这是对数据智能的前提。 总的来说，就是用智能改进传统行业，分两步   利用新的硬件收集传统行业的数据，量化传统行业的各个关键部分，对数据收集整理  通过数据对自身的反馈做出改变，并且用数据引领企业。难点：   传统行业，比如养殖业，农业，从事这些行业的人大多文化水平不高，对他们的数据难以收集","categories": ["extraction","reading"],
        "tags": ["AI"],
        "url": "http://localhost:4000/extraction/reading/%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3/"
      },{
        "title": "Machine Learning Tips",
        "excerpt":"Recap of Martin Zinkevich’s blog Best Practices for ML Engineering Before Machine Learning Do machine learning like the great engineer you are, not like the expert you aren't.  Machine learning is all of Engineering.ML is limited, but engineering is not. Engineering’s destiny is to quantify problems then solve it. In this case, the first target is how to quantify problems — Design metrics.   1: Don’t be afraid to launch a product without machine learning.  2: Make metrics design and implementation a priority.  3: Choose machine learning over a complex heuristic.    ML Phase I: Your First Pipeline     4: Keep the first model simple and get the infrastructure right.  5: Test the infrastructure independently from the machine learning.  6: Be careful about dropped data when copying pipelines.  7: Turn heuristics into features, or handle them externally.          stand on giant’s shoulders       Monitoring   8: Know the freshness requirements of your system.          How fast you will lost revenue with time goes by.How open you should update your model         9: Detect problems before exporting models.  10: Watch for silent failures.          Track statistics of Data, before preprocessing and feeds         11: Give feature sets owners and documentation.    Your First Objective     12: Don’t overthink which objective you choose to directly optimize.      13: Choose a simple, observable and attributable metric for your first objective.         14: Starting with an interpretable model makes debugging easier.     15: Separate Spam Filtering and Quality Ranking in a Policy Layer.    ML Phase II: Feature Engineering     16: Plan to launch and iterate.      17: Start with directly observed and reported features as opposed to learned features.     18: Explore with features of content that generalize across contexts.  19: Use very specific features when you can.  20: Combine and modify existing features to create new features in human understandable ways.  21: The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have.  22: Clean up features you are no longer using.          Unused features create technical debt       Human Analysis of the System   23: You are not a typical end user.  24: Measure the delta between models.  25: When choosing models, utilitarian performance trumps predictive power.  26: Look for patterns in the measured errors, and create new features.  27: Try to quantify observed undesirable behavior.          Quantify bad part and improve it. Again, Metric is the first important thingAt this point, they should do whatever it takes to turn their gripes into solid numbers.         28: Be aware that identical short term behavior does not imply identical long term behavior.    Training-Serving Skew           The best solution is to explicitly monitor it so that system and  data changes don’t introduce skew unnoticed.         29: The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time.  30: Importance weight sampled data, don’t arbitrarily drop it!  31: Beware that if you join data from a table at training and serving time, the data in the table may change.  32: Reuse code between your training pipeline and your serving pipeline whenever possible.  33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.          use next-day data         34: In binary classification for filtering (such as spam detection or determining interesting emails), make small short term sacrifices in performance for very clean data.  35: Beware of the inherent skew in ranking problems.  36: Avoid feedback loops with positional features.  37: Measure Training/Serving Skew.          Training data, validation data, test data(next data), live data.       ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models   38: Don’t waste time on new features if unaligned objectives have become the issue.  39: Launch decisions will depend upon more than one metric.  40: Keep ensembles simple.  41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.  42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.  43: Your friends tend to be the same across different products. Your interests tend not to be.","categories": ["Machine Learning","Reading","Recap"],
        "tags": ["Tips"],
        "url": "http://localhost:4000/machine%20learning/reading/recap/Machine-Learning-tips/"
      },{
        "title": "Distributed Tensorflow",
        "excerpt":"Terminology   Cluster: A cluster is composed of one or more Tensorflow servers, called tasks.  Job: a group of tasks that have a common roal. For instance, parameters server, worker server.  Task: A task corresponds to a specific TensorFlow server, and typically corresponds to a single process. A task belongs to a particular “job” and is identified by its index within that job’s list of tasks.    Table of Contents    Terminology  Multiple Devices on a Single Machine          Dependencies:      Managing GPU RAM      Placing operations on Devices        Multiple Devices Across Multiple Servers  Advantages:  Parallelizing Neural Network on a  Tensorflow Cluster          One Neural Network per device      In-Graph Versus Between-Graph Replication (ensemble)        Model Parallelism  Data Parallelism  Reference:Multiple Devices on a Single Machine Dependencies:   CUDA: Nvidia’s Compute Unified Device Architecture Library allows developers to call GPUs  cuDNN: CUDA deep neural network library to call CUDA  tensorflow-gpu    CUDA, cuDNN and Tensorflow Managing GPU RAM By default Tensorflow will grab all the RAM in all available GPUs the first time you run a graph, so if you run a second Tensorflow program, Memory error showing up.   One solution is to run each process  on different GPU card.# run script in specific GPU cardCUDA_VISIBLE_DEVICES = 0,1 python3 program.py  Another option is to set a threshold for Tensorflow, which limits the program can only grab a fraction of GPUs’ memory.config = tf.ConfigProto()config.gpu_option.per_process_gpu_memory_fraction = 0.4session = tf.Session(config=config)Placing operations on Devices Using the following code to place operation to a specific device, otherwise it will place to the default device.   Cautions: there is no way to pin nodes on a specific CPUs or a subset CPUS with tf.device(\"/cpu:0\"):    a = tf.Variable(3.0)Multiple Devices Across Multiple Servers   Create a Clusesrcluser_spec = tf.train.ClusterSpec({    \"worker\": [        \"machine-a.example.com:2222\", #/job:worker/task:0        \"machine-b.example.com:2222\" #/job:worker/task:1    ],    \"ps\": [        \"machine-a.example.com:2221\", #/job:ps/task:0    ]})    Distributed Tensorflow   Run a serverserver = tf.train.Server(cluster_spec, job_name=\"worker\",task_index=0)# start first worker task  Use server.join() to wait servers finish  In distribution tensorflow, variables maintain by resource container. In other words, we can access the variable across different sessions and servers. To avoid name clash, we can wrap variables by using tf.variable_scope or tf.container. tf.container is easy to reset and release variables.   Advantages:       Be able to explore a much larger hyperparameter space when fine-tuning your model    Large ensembles of NN efficiently  Parallelizing Neural Network on a  Tensorflow Cluster One Neural Network per device      One client per neural network per device.Each device running similar neural network with different hyperparameters.This solution is perfect for hyperparameter tuning. In-Graph Versus Between-Graph Replication (ensemble)   In-Graph: one client,one graph, one session maintains all stuff, one neural network per device.    In-Graph   Between-Graph: several clients maintain Input, Output and Neural Network. Each Neural network is an individual graph.    Between-GraphModel Parallelism   Separate Neural network Horizontally or Vertically.    Model ParallelismData Parallelism   Each device running same neural network, but feeded different mini-batch data. For each iteration, neural network fetch parameters from PS, then Gradients from each network aggrated to update parameters in PS. There are two main approach, synchronous and asynchronous.  With synchronous updates, the aggregator waits for all gradients to be available before computing the average and applying the result.The downside is faster replica have to wait slower one at every iteration. To make it more efficient, we can update parameters only if a fraction of replicas has finished.  With asynchronous updates, whenever a replica has finished computing the gradients, it immediately uses them to update the model parameters.    Data ParallelismReference:   Distributed Tensorflow: Tensorflow official doc  Echosystem: tensorflow official github project, a integration of tensorflow with other open-source framework  Hands-on Machine Learning with Scikit_Learn &amp; Tensorflow: Chapter 12, Distributing Tensorflow Across Devices and Server","categories": ["Machine Learning","Deep Learning"],
        "tags": ["Tensorflow","Distribution"],
        "url": "http://localhost:4000/machine%20learning/deep%20learning/distributed-tensorflow/"
      },{
        "title": "Products",
        "excerpt":"Dot Product(点积) Vector Product（叉乘，向量积，外积、叉积，矢积）   其方向与$\\vec{a} ,\\vec{b}$的平面垂直，且遵循右手定则 Poin-wise (逐点) ","categories": ["math"],
        "tags": ["math","linear algebra"],
        "url": "http://localhost:4000/math/products/"
      },{
        "title": "Reading Weekly",
        "excerpt":"            Date      Title      Author      Recap                  2018-04-26      Lessons from My First Two Years of AI Research      Tom Silver      No      Title: Lessons from My First Two Years of AI Research Date: 2018-04-26 Author: Tom Silver Recap:   ds  ds","categories": ["reading list"],
        "tags": ["weekly"],
        "url": "http://localhost:4000/reading%20list/readlist/"
      },{
        "title": "Layout: Post with Table Of Contents",
        "excerpt":"Enable table of contents on post or page by adding {% include toc %} where you’d like it to appear.     Table of Contents    HTML Elements  Body text          Blockquotes        List Types          Ordered Lists      Unordered Lists        Tables  Code Snippets  Buttons  NoticesHTML Elements Below is are some HTML elements. Check the source code to see the many embedded elements within paragraphs. Body text Lorem ipsum dolor sit amet, test link adipiscing elit. This is strong. Nullam dignissim convallis est. Quisque aliquam.  This is emphasized. Donec faucibus. Nunc iaculis suscipit dui. 53 = 125. Water is H2O. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. The New York Times (That’s a citation). Underline.Maecenas ornare tortor. Donec sed tellus eget sapien fringilla nonummy. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus. HTML and CSS are our tools. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus. Praesent mattis, massa quis luctus fermentum, turpis mi volutpat justo, eu volutpat enim diam eget metus. Blockquotes   Lorem ipsum dolor sit amet, test link adipiscing elit. Nullam dignissim convallis est. Quisque aliquam. List Types Ordered Lists   Item one          sub item one      sub item two      sub item three        Item twoUnordered Lists   Item one  Item two  Item threeTables             Header1      Header2      Header3                  cell1      cell2      cell3              cell4      cell5      cell6                  cell1      cell2      cell3              cell4      cell5      cell6                  Foot1      Foot2      Foot3      Code Snippets #container {  float: left;  margin: 0 -240px 0 0;  width: 100%;}Buttons Make any link standout more when applying the .btn class. &lt;a href=\"#\" class=\"btn btn--success\"&gt;Success Button&lt;/a&gt;Primary ButtonSuccess ButtonWarning ButtonDanger ButtonInfo ButtonNotices Watch out! You can also add notices by appending {: .notice} to a paragraph. ","categories": ["Layout"],
        "tags": ["table of contents"],
        "url": "http://localhost:4000/layout/tt/"
      },{
        "title": "少年",
        "excerpt":"你曾是少年\t\t\t\t\t\t\tYour browser does not support the audio element.\t\t\t你曾是少年，倔强勇敢不改变；你曾是少年，拌嘴吐槽也争先；你曾是少年，啤酒饺子鸡蛋面；你曾是少年，熬夜看书少睡眠；你曾是少年，周易老庄百家言；你曾是少年，江天万里不挂念；你曾是少年，物化生数都等闲；你曾是少年，追云逐日不自谦；你曾是少年，锦绣河山誓踏遍；你曾是少年，怒马扬尘花看厌；你曾是少年，转身告别不留恋。乘风破浪斩敌前，男儿至死是少年。  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/recipes/shao-nian/"
      }]
