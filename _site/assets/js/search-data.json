var store = [{
        "title": "Myself",
        "excerpt":"我 一个热爱历史，人文的工程师（💻🐒）。想要探索更多的世界和自己，记下美好的事物📸   古典武侠爱好者，喜欢金庸，古龙，尤其是古龙。  喜欢春秋战国时期的诸子百家思想，尤爱庄子，周易。  喜欢三国演义，看过最多遍的书了，8遍左右，不过比起主席的32遍还是差太远了🤪  铁杆漫威迷，海贼迷","categories": ["profile"],
        "tags": ["profile"],
        "url": "http://localhost:4000/profile/profile/"
      },{
        "title": "MathJax test Fourier Transform",
        "excerpt":"Fourier Transform: Inverse Fourier Transform: ","categories": ["math"],
        "tags": ["Fourier Transform"],
        "url": "http://localhost:4000/math/test/"
      },{
        "title": "PATH",
        "excerpt":"    Table of Contents    How to add commands to $PATH in Linux or Mac?          What is PATH?                  Option1:Set PATH for your current shell session          Option2:Change your PATH permanently                    How to add commands to $PATH in Linux or Mac? What is PATH? PATH is a variable in $ \\star$nix system which tells the command path Option1:Set PATH for your current shell session export PATH=$PATH:/path/to/your/directory  DoneOption2:Change your PATH permanently   Edit your .bash_profile(if you ues Bash) or .zshrc( if using zsh)  add path at the end of the fileexport PATH=$PATH:/path/to/your/directoryORexport PATH=/path/to/your/directory:$PATH  Active your changessource .bash_profileORsource .zshrc  Done","categories": ["linux"],
        "tags": ["path"],
        "url": "http://localhost:4000/linux/path/"
      },{
        "title": "Maximum Likelihood Estimation",
        "excerpt":"    Table of Contents    Bayesian Rule:  Maximum Likelihood Estimation (MLE)Bayesian Rule: $P(D \\mid \\theta) \\Rightarrow$ Likelihood $P(\\theta) \\Rightarrow$ prior $P(\\theta \\mid D) \\Rightarrow$ posterior $P(D) \\Rightarrow$ marginal likelihood Maximum Likelihood Estimation (MLE) AKA:最大似然估计，likelihood AKA: 似然函数 Apply log operation to avoid underflow For instance,if  ","categories": ["math"],
        "tags": ["MLE"],
        "url": "http://localhost:4000/math/mle/"
      },{
        "title": "PCA",
        "excerpt":"    Table of Contents    方差  协方差  协方差矩阵  算法及实例          PCA算法      实例      进一步讨论      Blog Source: 方差 上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即： 由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示： 于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。 协方差 对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。 数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则： 可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。 当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。 至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 协方差矩阵 上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。 我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X： 然后我们用X乘以X的转置，并乘上系数$1/m$： 奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。 根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况： 设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。 ##协方差矩阵对角化根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系： 设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系： 现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。 至此，我们离“发明”PCA还有仅一步之遥！ 现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。 由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质： 1）实对称矩阵不同特征值对应的特征向量必然正交。 2）设特征向量重数为r，则必然存在r个线性无关的特征向量对应于\\lambda，因此可以将这r个特征向量单位正交化。 由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为，我们将其按列组成矩阵： 则对协方差矩阵C有如下结论： 其中为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。 以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。 到这里，我们发现我们已经找到了需要的矩阵P： P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。 至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。 算法及实例 为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。 PCA算法 总结一下PCA的算法步骤： 设有m条n维数据。 1）将原始数据按列组成n行m列矩阵X 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 3）求出协方差矩阵 4）求出协方差矩阵的特征值及对应的特征向量 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P 6）Y=PX即为降维到k维后的数据 实例 这里以上文提到的 为例，我们用PCA方法将这组二维数据其降到一维。 因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵： 然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为： 其对应的特征向量分别是： 其中对应的特征向量分别是一个通解，可取任意实数。那么标准化后的特征向量为： 因此我们的矩阵P是： 可以验证协方差矩阵C的对角化： 最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示： 降维投影结果如下图:     PCA image.进一步讨论 根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。 因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。 最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。 希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。 ","categories": ["machine learning"],
        "tags": ["math","PCA","dimension reduction"],
        "url": "http://localhost:4000/machine%20learning/pca/"
      },{
        "title": "智能时代",
        "excerpt":"智能时代 出版时间:2017作者:吴军正如书名一样，书里描绘里一个智能的时代。或者说是一个数据驱动的时代。科技的进步来源于人类对自然或或者说是事物的认知。也可以换一种说法，是来源于数据。人类的进步就是数据的积累。人们不断的从数据里总结，学习。从而才能产生知识—-一种对数据加工而得到的结论。人类的早期对数据的收集都比较慢，即没有工具，也没有处理的方法。这也就造成了人类的认知不足。随着时间的发展，数据量渐渐增大，人类可以从数据中提炼，挖掘出有效的信息，根据先验知识或者是由数据驱动。这就是我的工作，有效的挖掘数据的潜在价值，利用数据提取信息，进而用这些信息做一些事。所谓智能，不过是把数据中的信息挖掘出智能。比方说图像分类，人脸识别，声音识别。都是来源于数据，并且对数据加工，提取数据的共性，并且放大那些决定性的差异，然后才进行分类。另外一方面也十分重要，那就是对数据的收集。或者说是制造数据。这是对数据智能的前提。 总的来说，就是用智能改进传统行业，分两步   利用新的硬件收集传统行业的数据，量化传统行业的各个关键部分，对数据收集整理  通过数据对自身的反馈做出改变，并且用数据引领企业。难点：   传统行业，比如养殖业，农业，从事这些行业的人大多文化水平不高，对他们的数据难以收集","categories": ["extraction","reading"],
        "tags": ["AI"],
        "url": "http://localhost:4000/extraction/reading/%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3/"
      },{
        "title": "Machine Learning Tips",
        "excerpt":"Recap of Martin Zinkevich’s blog Best Practices for ML Engineering Before Machine Learning Do machine learning like the great engineer you are, not like the expert you aren't.  Machine learning is all of Engineering.ML is limited, but engineering is not. Engineering’s destiny is to quantify problems then solve it. In this case, the first target is how to quantify problems — Design metrics.   1: Don’t be afraid to launch a product without machine learning.  2: Make metrics design and implementation a priority.  3: Choose machine learning over a complex heuristic.    ML Phase I: Your First Pipeline     4: Keep the first model simple and get the infrastructure right.  5: Test the infrastructure independently from the machine learning.  6: Be careful about dropped data when copying pipelines.  7: Turn heuristics into features, or handle them externally.          stand on giant’s shoulders       Monitoring   8: Know the freshness requirements of your system.          How fast you will lost revenue with time goes by.How open you should update your model         9: Detect problems before exporting models.  10: Watch for silent failures.          Track statistics of Data, before preprocessing and feeds         11: Give feature sets owners and documentation.    Your First Objective     12: Don’t overthink which objective you choose to directly optimize.      13: Choose a simple, observable and attributable metric for your first objective.         14: Starting with an interpretable model makes debugging easier.     15: Separate Spam Filtering and Quality Ranking in a Policy Layer.    ML Phase II: Feature Engineering     16: Plan to launch and iterate.      17: Start with directly observed and reported features as opposed to learned features.     18: Explore with features of content that generalize across contexts.  19: Use very specific features when you can.  20: Combine and modify existing features to create new features in human understandable ways.  21: The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have.  22: Clean up features you are no longer using.          Unused features create technical debt       Human Analysis of the System   23: You are not a typical end user.  24: Measure the delta between models.  25: When choosing models, utilitarian performance trumps predictive power.  26: Look for patterns in the measured errors, and create new features.  27: Try to quantify observed undesirable behavior.          Quantify bad part and improve it. Again, Metric is the first important thingAt this point, they should do whatever it takes to turn their gripes into solid numbers.         28: Be aware that identical short term behavior does not imply identical long term behavior.    Training-Serving Skew           The best solution is to explicitly monitor it so that system and  data changes don’t introduce skew unnoticed.         29: The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time.  30: Importance weight sampled data, don’t arbitrarily drop it!  31: Beware that if you join data from a table at training and serving time, the data in the table may change.  32: Reuse code between your training pipeline and your serving pipeline whenever possible.  33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.          use next-day data         34: In binary classification for filtering (such as spam detection or determining interesting emails), make small short term sacrifices in performance for very clean data.  35: Beware of the inherent skew in ranking problems.  36: Avoid feedback loops with positional features.  37: Measure Training/Serving Skew.          Training data, validation data, test data(next data), live data.       ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models   38: Don’t waste time on new features if unaligned objectives have become the issue.  39: Launch decisions will depend upon more than one metric.  40: Keep ensembles simple.  41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.  42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.  43: Your friends tend to be the same across different products. Your interests tend not to be.","categories": ["Machine Learning","Reading","Recap"],
        "tags": ["Tips"],
        "url": "http://localhost:4000/machine%20learning/reading/recap/Machine-Learning-tips/"
      },{
        "title": "Distributed Tensorflow",
        "excerpt":"Terminology   Cluster: A cluster is composed of one or more Tensorflow servers, called tasks.  Job: a group of tasks that have a common roal. For instance, parameters server, worker server.  Task: A task corresponds to a specific TensorFlow server, and typically corresponds to a single process. A task belongs to a particular “job” and is identified by its index within that job’s list of tasks.    Table of Contents    Terminology  Multiple Devices on a Single Machine          Dependencies:      Managing GPU RAM      Placing operations on Devices        Multiple Devices Across Multiple Servers  Advantages:  Parallelizing Neural Network on a  Tensorflow Cluster          One Neural Network per device      In-Graph Versus Between-Graph Replication (ensemble)        Model Parallelism  Data Parallelism  Reference:Multiple Devices on a Single Machine Dependencies:   CUDA: Nvidia’s Compute Unified Device Architecture Library allows developers to call GPUs  cuDNN: CUDA deep neural network library to call CUDA  tensorflow-gpu    CUDA, cuDNN and Tensorflow Managing GPU RAM By default Tensorflow will grab all the RAM in all available GPUs the first time you run a graph, so if you run a second Tensorflow program, Memory error showing up.   One solution is to run each process  on different GPU card.# run script in specific GPU cardCUDA_VISIBLE_DEVICES = 0,1 python3 program.py  Another option is to set a threshold for Tensorflow, which limits the program can only grab a fraction of GPUs’ memory.config = tf.ConfigProto()config.gpu_option.per_process_gpu_memory_fraction = 0.4session = tf.Session(config=config)Placing operations on Devices Using the following code to place operation to a specific device, otherwise it will place to the default device.   Cautions: there is no way to pin nodes on a specific CPUs or a subset CPUS with tf.device(\"/cpu:0\"):    a = tf.Variable(3.0)Multiple Devices Across Multiple Servers   Create a Clusesrcluser_spec = tf.train.ClusterSpec({    \"worker\": [        \"machine-a.example.com:2222\", #/job:worker/task:0        \"machine-b.example.com:2222\" #/job:worker/task:1    ],    \"ps\": [        \"machine-a.example.com:2221\", #/job:ps/task:0    ]})    Distributed Tensorflow   Run a serverserver = tf.train.Server(cluster_spec, job_name=\"worker\",task_index=0)# start first worker task  Use server.join() to wait servers finish  In distribution tensorflow, variables maintain by resource container. In other words, we can access the variable across different sessions and servers. To avoid name clash, we can wrap variables by using tf.variable_scope or tf.container. tf.container is easy to reset and release variables.   Advantages:       Be able to explore a much larger hyperparameter space when fine-tuning your model    Large ensembles of NN efficiently  Parallelizing Neural Network on a  Tensorflow Cluster One Neural Network per device      One client per neural network per device.Each device running similar neural network with different hyperparameters.This solution is perfect for hyperparameter tuning. In-Graph Versus Between-Graph Replication (ensemble)   In-Graph: one client,one graph, one session maintains all stuff, one neural network per device.    In-Graph   Between-Graph: several clients maintain Input, Output and Neural Network. Each Neural network is an individual graph.    Between-GraphModel Parallelism   Separate Neural network Horizontally or Vertically.    Model ParallelismData Parallelism   Each device running same neural network, but feeded different mini-batch data. For each iteration, neural network fetch parameters from PS, then Gradients from each network aggrated to update parameters in PS. There are two main approach, synchronous and asynchronous.  With synchronous updates, the aggregator waits for all gradients to be available before computing the average and applying the result.The downside is faster replica have to wait slower one at every iteration. To make it more efficient, we can update parameters only if a fraction of replicas has finished.  With asynchronous updates, whenever a replica has finished computing the gradients, it immediately uses them to update the model parameters.    Data ParallelismReference:   Distributed Tensorflow: Tensorflow official doc  Echosystem: tensorflow official github project, a integration of tensorflow with other open-source framework  Hands-on Machine Learning with Scikit_Learn &amp; Tensorflow: Chapter 12, Distributing Tensorflow Across Devices and Server","categories": ["Machine Learning","Deep Learning"],
        "tags": ["Tensorflow","Distribution"],
        "url": "http://localhost:4000/machine%20learning/deep%20learning/distributed-tensorflow/"
      },{
        "title": "Products",
        "excerpt":"Dot Product(点积) Vector Product（叉乘，向量积，外积、叉积，矢积）   其方向与$\\vec{a} ,\\vec{b}$的平面垂直，且遵循右手定则 Poin-wise (逐点) ","categories": ["math"],
        "tags": ["math","linear algebra"],
        "url": "http://localhost:4000/math/products/"
      },{
        "title": "Reading Weekly",
        "excerpt":"            Date      Title      Author      Recap                  2018-04-26      Lessons from My First Two Years of AI Research      Tom Silver      No      Title: Lessons from My First Two Years of AI Research Date: 2018-04-26 Author: Tom Silver Recap:   ds  ds","categories": ["reading list"],
        "tags": ["weekly"],
        "url": "http://localhost:4000/reading%20list/readlist/"
      },{
        "title": "Layout: Post with Table Of Contents",
        "excerpt":"Enable table of contents on post or page by adding {% include toc %} where you’d like it to appear.     Table of Contents    HTML Elements  Body text          Blockquotes        List Types          Ordered Lists      Unordered Lists        Tables  Code Snippets  Buttons  NoticesHTML Elements Below is are some HTML elements. Check the source code to see the many embedded elements within paragraphs. Body text Lorem ipsum dolor sit amet, test link adipiscing elit. This is strong. Nullam dignissim convallis est. Quisque aliquam.  This is emphasized. Donec faucibus. Nunc iaculis suscipit dui. 53 = 125. Water is H2O. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. The New York Times (That’s a citation). Underline.Maecenas ornare tortor. Donec sed tellus eget sapien fringilla nonummy. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus. HTML and CSS are our tools. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus. Praesent mattis, massa quis luctus fermentum, turpis mi volutpat justo, eu volutpat enim diam eget metus. Blockquotes   Lorem ipsum dolor sit amet, test link adipiscing elit. Nullam dignissim convallis est. Quisque aliquam. List Types Ordered Lists   Item one          sub item one      sub item two      sub item three        Item twoUnordered Lists   Item one  Item two  Item threeTables             Header1      Header2      Header3                  cell1      cell2      cell3              cell4      cell5      cell6                  cell1      cell2      cell3              cell4      cell5      cell6                  Foot1      Foot2      Foot3      Code Snippets #container {  float: left;  margin: 0 -240px 0 0;  width: 100%;}Buttons Make any link standout more when applying the .btn class. &lt;a href=\"#\" class=\"btn btn--success\"&gt;Success Button&lt;/a&gt;Primary ButtonSuccess ButtonWarning ButtonDanger ButtonInfo ButtonNotices Watch out! You can also add notices by appending {: .notice} to a paragraph. ","categories": ["Layout"],
        "tags": ["table of contents"],
        "url": "http://localhost:4000/layout/tt/"
      },{
        "title": "Use jupyter on remote server",
        "excerpt":"If you want to use jupyter on remote server, this blog might be useful. Step1:Run jupyter without browser on  remote server: jupyter notebook --no-browser --port=8888Step2:Bind local port to remote server: ssh -N -f -L localhost:8888:ip:8888 user@ipStep3:Open browser on you local machine and direct to localhost:8888 To Stop Local Process: ps aux|grep jupyterkill pid","categories": ["tools"],
        "tags": ["jupyter"],
        "url": "http://localhost:4000/tools/remote-jupyter/"
      },{
        "title": "Connect server with spawn and expect",
        "excerpt":"expect spawn and send #!/usr/bin/expect -f#set timeout 20set sp YourSudoPasswordset passwd UserPassword# sudo is not always necessaryspawn sudo ssh user@ipexpect \"Password:\"send \"$sp\\r\"expect \"some stuff\"send \"$passwd\\r\"interact  spawn: run commands  expect: wait for specify pattern  send: to send the strings to the process","categories": ["tools"],
        "tags": ["ssh","spawn","server"],
        "url": "http://localhost:4000/tools/ssh-connect/"
      },{
        "title": "Some Usefull Tools  in Apple Ecosystem",
        "excerpt":"Sharing some usefull apple apps In Both IOS and MAC OS:   Things  Fantastical 2  Notability  Reeder  Day One  Duet  Alfread  Bear  MacDown  CheatSheet  MindnoteOnly in MAC OS:   Quiver  Break Time  Alfread  VLC  Rescue Time  NoizioOnly in IOS:   VSCO  Tailor  intiLive  inpaint  pic Sticher  shazam","categories": ["tools"],
        "tags": ["apple apps"],
        "url": "http://localhost:4000/tools/apple-tools/"
      },{
        "title": "富爸爸穷爸爸",
        "excerpt":"成书时间:2000作者:罗伯特∙清崎第一章：富人不为钱工作   避开一生中最大的陷阱，乱消费。否则收入越高则消费越高。  忘记自己的工资，别像只驴子一样盯着眼前的胡萝卜🥕。要认清自己想要的。  让感情跟随思想，而不是思想跟随感情。用头脑思考，不受感情控制。第二章：为什么要教授财务知识   明确区分资产和负债，购买资产，减少负债。  资产是把钱放进口袋的，而负债是把钱从口袋拿出去的。  创造现金流，用现金流购买  财富是将资产项产生的现金与支出项流出的现金对比而定第三章：关注自己的事业   获得自己喜欢的，了解的资产第四章：税收的历史和公司的力量   利用公司，合理避税第五章：富人的投资   投资自身的教育，学会更多技能  学习会计，投资，了解市场，法律第六章：学会不为钱工作   不断学习才是一切第七章：客服困难   客服不良习惯，恐惧第八章：开始行动   永远在路上，每天都是一步  总结：就是个披着理财皮的励志鸡汤书，废话连篇，啰里八嗦。就只有前两章有点启发。 ","categories": ["reading"],
        "tags": ["finance"],
        "url": "http://localhost:4000/reading/rich-dad/"
      },{
        "title": "why small teams win",
        "excerpt":"why small teams win 36氪翻译版：   小团队因为能够快速有效地协调。最重要的是，小团队擅长创造让人关心的环境，在这种环境中，个人的贡献是可以观察到的，而且是有价值的。成功产品的最终命运是它们能很快吸引更多的人来使用它。当我们让更多的人参与项目时，我们需要确保责任、自主权和意义保持不变。这是我们在之前的铅笔例子中看到的。当每个人都创造出他们可以观察和理解的价值时，他们就能有效地运作。换句话说，我们需要在扩大团队规模之前明确为何团队要扩张。一旦我们让人们的个人贡献和责任被稀释，产品就离变得平庸不远了。 每个人都要创造可被观察到的，有价值的贡献 ","categories": ["reading"],
        "tags": ["teams","efficiency"],
        "url": "http://localhost:4000/reading/why-small-teams-win/"
      },{
        "title": "颠覆式创新",
        "excerpt":"颠覆式创新   李善友       小公司要想打败大公司必须在大公司的盲区，做大些大公司不会，不屑做的事    公司要想长久存在，必须保持生命力，生长力。破坏，再创新。    公司的对手应该是这个时代，不随着时代进步的公司必定会死亡    有些事，我们不做就一定会有别人做，别人做了就不会有我们的余地了    保持简洁，简单，小而精的模式    永远保持生长的状态，安逸必定会死亡（生于忧患，死于安乐）    另外书中反复提到两本书，一，创新者的窘境，二，疯狂的简洁 ","categories": ["extraction","reading"],
        "tags": ["enterpries"],
        "url": "http://localhost:4000/extraction/reading/%E9%A2%A0%E8%A6%86%E5%BC%8F%E5%88%9B%E6%96%B0/"
      },{
        "title": "PRINCIPLES",
        "excerpt":"PRINCIPLES🦉 Ray Dalio 生活的原则   直面严酷的现实，做个超现实的人，确切的规划出现实通往梦想的路  直面痛苦，快速迭代自己，进化  从更高层次俯视自己，把自己想成一个机器，生活想成游戏。  不要受控于自己的情感，凭理智和逻辑行动工作的原则   任何人不应大于体系，离开任何人，体系必须要能正常运行  任何人都应该受到监督，向人汇报  公司应该像机器，好的管理者要像工程师去发现公司的问题，查找问题的根源，解决问题。  公司应有章可循，有规则  公司要尽量透明，紧密，同时关系要融洽。  透明是有限度，分人，分事的  朴素主义 ","categories": ["extraction","reading"],
        "tags": ["enterpries"],
        "url": "http://localhost:4000/extraction/reading/principles/"
      },{
        "title": "断舍离",
        "excerpt":"断舍离 山下英子 断$\\bigoplus$舍$ \\Longrightarrow$离 断： 不添加不需要的 舍： 清除放弃不需要的 离： 脱离执念   认清自己真正的需求，真正要的是什么。无论是物质上，还是精神上。只有拨开迷雾，才能找到真实。物质上的单纯也会导致思维上的愉悦与活力。 ","categories": ["extraction","reading"],
        "tags": ["enterpries","life"],
        "url": "http://localhost:4000/extraction/reading/%E6%96%AD%E8%88%8D%E7%A6%BB/"
      },{
        "title": "密度聚类",
        "excerpt":"密度聚类假设：聚类结构能通过样本分布的紧密程度来确认 密度聚类核心概念：设给定数据集$D={x_1, x_2,x_3 \\cdots x_m }$ $\\epsilon$领域：对于$x_j \\in D $， $x_j$ 的$\\epsilon$领域内的数据点集合为： 核心对象：若$|N_{\\epsilon}(x_j) | \\geq P_{min}$，则 $x_j$为核心对象 密度直达：若$x_i$在$x_j$的领域内，且，$x_j$为核心对象，则称$x_i$为$x_j$的密度直达。 密度可达：若$x_i$可以通过$x_j$一条，一条的密度直达连接，则称$x_j$为$x_j$的密度可达。 密度相连：若存在某点$x_k$使$x_i, x_j$均为$x_k$的密度可达，则称$x_i, x_j$为密度相连。 密度簇：通过密度可达导出的集合，集合内任意一点都能在此集合内找到另外一点是此点的密度直达或能密度直达到此点的点。集合外不存在与簇内密度相连的点。不属于任何簇的点为噪声或异常值 定义距离： 金额和时间在距离的影响是不等价的，时间优先。同时鉴于只有两维数据，采用乘法连接两方权重，此方法能把特征空间非线性的映射到更大的距离空间，而且数据只有两维，不会因为连乘导致上溢。维度多的数据不适用此距离公式。 $x_i =(amount,time)_i = (a_i,t_i)$ 加入权重（1，0）消除金额差为0时的导致时间效用为0的缺点，始终保证时间的效应存在并影响距离 或可采用： 密度聚类样板代码 import numpy as npfrom sklearn.datasets import load_irisfrom functools import reducedef dist(x):\t\"\"\"\tDistance = |a-b|+weight\t:param x: np.array[n_points * features]\t:return: np.array[n_points * n_points]\t\"\"\"\tleng = x.shape[0]\tweight = np.array([0, 0, 0, 1])\tdis_map = np.zeros((leng, leng))\tfor i in range(leng):\t\tfor j in range(leng):\t\t\tdis_map[i, j] = np.prod(np.abs(x[i] - x[j]) + weight)\treturn dis_map\tif __name__ == \"__main__\":\tdata = load_iris()\tX = data.data[:100]\tY = data.target[:100]\ttheta = 0.1\tminp = 30\tdis_map = dist(X)\tcores = dis_map.copy()\tcores[cores &gt; theta] = 0\tcore_list = []\tn_reachables = np.sum(np.sign(cores), 1)\tfor i, j in enumerate(n_reachables):\t\tif j &gt; minp:\t\t\tcore_list.append(i)\tcores_set = set(core_list)\tvisited = set()\tclusters = []\twhile cores_set:\t\tss = cores_set.pop()\t\tque = set([ss])\t\tpre_visited = visited.copy()\t\twhile que:\t\t\tj = que.pop()\t\t\tvisited.update([j])\t\t\tif j in core_list:\t\t\t\tc = np.argwhere(cores[j] &gt; 0)\t\t\t\tad_join = reduce(lambda x, y: x + y, c.tolist())\t\t\t\tque.update(set(ad_join) - visited)\t\tclusters.append(visited - pre_visited)\t\tcores_set = cores_set - visited","categories": ["mechine learning","clustering"],
        "tags": ["math","mechine learning"],
        "url": "http://localhost:4000/mechine%20learning/clustering/density_based_clustering/"
      },{
        "title": "黑天鹅",
        "excerpt":"一半对一半  如何与黑天鹅打成平手 黑天鹅，如何预知不可预知的未来 一半时间里，我是一个超级怀疑主义者；另一半时间，我又坚定不移地相信事物的确定性. 而且是非常顽固地相信.  当其他人， 尤其是那些被我称为文化市侩的人持轻信态度的时候，我是超级怀疑主义者；当其他人看上去持怀疑态度的时候，我是轻信者。 我对证实的事物持怀疑态度. 但只是在错误的代价很高的时候. 而对证伪的事物不持怀疑态度。 掌握大量数据并不一定能够证实什么，但一个个例就可以证伪. 当我怀疑存在疯狂随机性时，我保持怀疑态度；当我认为只存在温和随机性时， 我选择相信. 一半时间我讨厌黑天鹅. 另一半时间我热爱黑天鹅。我喜欢为生活带来细节、 正面意外、 画家阿佩勒斯的成功， 不必花钱的礼物的随机性。 很少有人理解阿佩勒斯故事中的美。 实际上，大部分人通过压抑自己体内的阿佩勒斯来避免犯错. 一半时间我对自己的事务超级保守q，另邂一半时间我超级冒险. 这似平并没有什么特别， 只不过我在其他入冒险的地方实行保守主义， 在其他人谨慎的地方冒险. 我不怎么在意小的失败，而是在意大的终极性失败.  我更担心”极具前景“的股栗市场. 尤其是 ”安全的” 蓝筹股. 而不是从事投机的公司，前者代表看不见的风险, 后者则不会造成意外l 因为你知道它们的波动性有多大，并且可以通过进行小额没资来控制亏损面。 我不撞心广为人知和耸人昕闻的风险. 而担心更为险恶的隐藏风险. 我不担心恐怖主又. 而担心糖尿。 我不担心人们通常担心的问题，因为它们显而易见，而担心我们的意识和正常过程以外的事物. (我也必须承认我担心的东西不多，因为我努力只担心有计可施的事情) 我不担心困境. 而担心失去机会。 最终存在一个小小的决策法则: 当我能够受到正面黑天鹅事件影响时.我会非常冒睑，这时失败只有很小的影响；当我有可能受到负面黑天鹅事件的袭击时，我会非常保守， 当某个模型中的错误对我有好处时. 我会非常冒险，当错误对我有害时，我会非常多疑. 这可能并不十分有趣. 但这正是别人没有做到的。例如，在金融业，人们使用脆弱的理论来管理风险、 把狂野的思想置于 ”理性“的审视之下。 一半时间我是思想者，另一半时间我是一个理智的实践者. 我对学术问题保持理智和务实，对实际问题保持哲学思考。 一半时间我很肤浅、 另一半时间我想避免肤浅. 对于美学我很肤浅，对于风险和回报我避兔肤浅。 我的唯美主义让我把诗歌置二散文之上、 把希腊人置干罗马人之上、 把尊严置于优雅之上、 把优雅置于文化之上， 把文化置于学识之上，把学识置于知识之上. 把知识置于智力之山 把智力置干真理之上， 但这一区别只是针对不受黑天鹅影响的事. 我们的本性喜坎理性，除了面对黑天鹅的时候。 我认识的一半人称我对权威不敬 (你己经看到我对那些柏拉图化的教授们的怦帆 一半人称我对权威奉承 (你己经看到我对休持、 拜耳、 波普尔麒彭加菜、  哈耶克禾口其他人的崇拜)。 一半时间我讨厌尼采，另一半肘间我喜欢他的散文。 何时错过列车没有痛苦 我曾经得到 另一条改变生活的建议，与我第三章从朋友那里得到的建议不同，它实用、 明智、 有效. 我在巴黎的同学，后来的小悦家让一奥利维尔 ‘ 泰德斯克 认为我不必跑着赶地铁， 他说: “我不会去追赶列车.” 藐视命运。我一直教我自己拒绝追赶时间表。这可能只是一条很小的建但它印在了我的脑悔里. 从拒绝追赶列车中， 我体会到 优雅的真正价值和行为中的美学，一种控制我的时间，行程和生活的感觉. 错过列车、只有在你追赶它时才是痛苦的！同样， 不能达到别人对你期望的成功. 只有它也是你所追求的东西时才是痛苦的. 你凌驾于争斗与名利思想之上，而不是之外，只要你愿意这样选择 只要是你的决定， 放弃一份高薪职位带来的回报会超过金钱带绐你的效用 (这似平很疯狂， 但我试过并且确实如此） 这是向命运说出，“去XX的” 的第一步. 如果你确定了自己的标准、 你对自己的生活会有大得多的控制。 大自然给了我们一些防御机制: 如伊索寓言中写到的一桦 其中之一就是把我们不能 (或没有) 吃到的葡萄想成酸的。 但如果你更加主动地在之前就鄙视并拒绝葡荀、 你的满足感会更大. 主动出击， 主动辞职， 只要你有勇气。 你在一个自己设计的游戏里更加难以失败. 从黑天鹅的角度讲，这意味着只有小概率事件控制你的时候，你才会受到它的影晌。 你要总是控制你自己做什么 把这当成你的目标吧. 不过，所有这些思想， 所有这些归纳的哲学， 所有这些知识问题， 所有这些疯狂的机会和可怕的损失、 在下面这个形而上学的问题面前都变得简单.我有时惊异于为什么人们会因为一顿不好吃的饭、 一杯冷咖啡、 一次社交挫折或粗鲁的接待而伤心一天或者感到愤怒， 回忆一下、 我在第八章讨论过人们难以看到自己生活中发生的事件的真实慨率.我们很容易忘记我们活着本身就是极大的运气， 一个可能性微小的事件， 一个极大的偶然.想象一个10亿倍于地球的行星边上的一粒尘埃。 这粒尘埃就代表你出生的概串，庞大的行星则代表相反的概率. 所以不要再为小事烦恼了，不要再像一个忘恩负义者、 得到一座城堡. 还要介意浴室里的霉菌. 不要再捡查别人赠与你的马匹的牙齿， 请记住， 你就是黑天鹅。 ","categories": ["extraction","reading"],
        "tags": ["Finance"],
        "url": "http://localhost:4000/extraction/reading/black-swan/"
      },{
        "title": "随机漫步的傻瓜",
        "excerpt":"随机现象与优雅行为 随机漫步的傻瓜 关于人生在世应该有什么样的行为， 读者应该已经知道我会怎么建议和如何说教。 我们说过， 人有情绪反应时， 合乎理性的观念很难听进耳里; 走出教室， 我们就不会用到理性的大脑。 一些自助励志书即使不是胡说八道的人写的， 大致也没什么效果。 启发人心而且  友善的好建议和具说服力的说教， 如果不能拨动我们的心弦， 便将稍纵即逝。 斯多噶哲学有趣的地方，在于它强调尊严和个人的美感， 而这是我们基因中的一部分。 下次碰到厄运时， 不妨开始强调个人举止的优雅。 你应该表现出不管在什么状况下， 都道如何生存” 。 行刑日那天把最好的衣服穿上 (仔细刮好胡子)； 挺直腰杆站直， 显现一股傲气， 好在行刑队心里留下美好的印象。 诊断出罹患癌症时， 不要哭天喊地， 一副无辜受害的样子。只和医生讨论病情， 切莫让别人知道， 如此就可避免听到老掉牙的安慰话， 也没人会视你为值得同情的受害人； 此外， 那种有尊严的态度， 可以让挫败和胜利一样， 都叫人觉得具有英雄气概。 赔钱的时候， 务必对你的助理吏为客气， 不要对他发怒 (许多交易员经常这个祥子，令人不齿)。 不要将你的命运怪罪于任何人， 即使他们确实是祸首也是一样。 就算你的另一半和英俊的滑雪教练或年轻但野心不小的模特儿搞上， 也绝不要自怜自艾。 别怨东怨西。 如果你的生意变少， 不要马上哈腰屈膝， 可以像我儿时的好友艾波史雷曼那祥， 发出一封充满英雄气概的电子邮件给同行， 告诉他们：“生意虽少，  命运女神唯一不能控制的东西， 是你的行为 。 祝你好运! ","categories": ["extraction","reading"],
        "tags": ["Finance"],
        "url": "http://localhost:4000/extraction/reading/randomness/"
      },{
        "title": "于我，过去、现在和未来",
        "excerpt":"作者：西格里夫·萨松  In me, past, present, future meet To hold long chiding conference. My lusts usurp the present tense And strangle Reason in his seat. My loves leap through the future’s fence To dance with dream-enfranchised feet. In me the cave-man clasps the seer, And garlanded Apollo goes Chanting to Abraham’s deaf ear. In me the tiger sniffs the rose. Look in my heart, kind friends, and tremble, Since there your elements assemble.  于我，过去、现在和未来 商讨聚会 各执一词 纷扰不息。 林林总总的 欲望，掠取着我的现在 把“理性”扼杀于它的宝座 我的爱情纷纷越过未来的藩篱 梦想解放出它们的双脚 舞蹈不停 于我，穴居人攫取了先知， 佩戴花环的阿波罗神 向亚伯拉罕的聋耳唱叹歌吟。 心有猛虎，细嗅蔷薇。 审视我的内心吧，亲爱的朋友，你应颤栗， 因为那才是你本来的面目。 ","categories": ["reading"],
        "tags": ["life"],
        "url": "http://localhost:4000/reading/forme/"
      },{
        "title": "K-Means聚类",
        "excerpt":"K-Means 聚类 矩阵形式 class KMeans:\t\"\"\"cash tracker a K-MEANS clustering approach  Version 0\"\"\"\tdef __init__(self, x, n_classes):\t\t\"\"\"centroid initialization\t\tcentroid: [n_classes * features]\t\tX: [n_points * features]\t\tclusters: [n_classes * n_points]\t\tresult: [n_classes * n_points]\t\t\"\"\"\t\tind = random.sample(range(x.shape[0]), n_classes)\t\tself.centroids = x[ind]\t\tself.X = x.T\t\tself.n_classes = n_classes\t\tself.clusters = None\t\tself.result = None\t\tself.b2 = np.sum(np.multiply(self.X, self.X), 0).reshape(1, -1)\tdef update_centroids(self):\t\t\"\"\"update centroids\t\tIn this approach, a single point may belong to two different clusters at the same time\t\tonly if the point's distance score stay the same in the two clusters\t\t\"\"\"\t\tmask = 1 + np.sign(self.clusters.min(0)-self.clusters)\t\tdivisor = np.sum(mask, axis=1).reshape(self.n_classes, 1) + 1\t\tself.centroids = np.divide(np.dot(mask, self.X.T), divisor)\t\tself.result = mask\tdef update_distance(self):\t\t\"\"\"update clusters Euclidean distance= sqrt((a-b)^2)= sqrt(a^2-2ab+b^2)\t\tIn this case, A matrix form  is using to accelerate computation\t\t\"\"\"\t\ta2 = np.sum(np.multiply(self.centroids, self.centroids),1).reshape(-1,1)\t\tself.clusters = np.sqrt(a2-2*np.dot(self.centroids, self.X) + self.b2)\t@staticmethod\tdef silhouette_coef(x, results):\t\t\"\"\"\t\t:param x: np.array[n_points * features]\t\t:param results: np.array[n_classes * points]\t\t:return: int\t\t\"\"\"\t\tn_classes = results.shape[0]\t\tpoints_map = KMeans.euclidean_metric(x)\t\tcluster_map = np.dot(results, points_map)\t\tn_inner_points = np.sum(results, axis=1).reshape(n_classes, 1) + 1\t\tavg_cluster_map = np.divide(cluster_map, n_inner_points)\t\tinner_dist = np.sum(np.multiply(results, avg_cluster_map), 0)\t\texter_dist = np.multiply(1 - results, avg_cluster_map)\t\texter_dist[exter_dist == 0] = np.nan\t\tb=np.nanmin(exter_dist, axis=0)\t\tsil = np.divide(b-inner_dist,np.fmax(inner_dist,b))\t\treturn np.mean(sil)\t@staticmethod\tdef euclidean_metric(a, b=None):\t\t\"\"\"\t\tEuclidean Metric = sqrt((a-b)^2)= sqrt(a^2-2ab+b^2)\t\tA matrix form  is using to accelerate computation\t\t:param a: np.array[n_points * features]\t\t:param b: np.array[features * m_points]\t\t:return: np.array[n_points * m_points]\t\t\"\"\"\t\tif not b:\t\t\tb = a.T\t\tv = np.sum(np.multiply(a, a), 1).reshape(-1, 1) - 2 * np.dot(a, b) + np.sum(np.multiply(b, b), 0).reshape(1, -1)\t\treturn v\t@staticmethod\tdef external_validity_index(predict, target):\t\t\"\"\"\t\tExternal Validity Index\t\t:param predict: outputs\t\t:param target: targets\t\t:return: float\t\t\"\"\"\t\tss, sd, ds, dd = [], [], [], []\t\tfor i in range(len(predict)):\t\t\tfor j in range(i+1, len(predict)):\t\t\t\tif predict[i] == predict[j]:\t\t\t\t\tif target[i] == target[j]:\t\t\t\t\t\tss.append((i, j))\t\t\t\t\telse:\t\t\t\t\t\tsd.append((i, j))\t\t\t\telse:\t\t\t\t\tif target[i] == target[j]:\t\t\t\t\t\tds.append((i, j))\t\t\t\t\telse:\t\t\t\t\t\tdd.append((i, j))\t\treturn len(ss)/len(ss+sd+ds)","categories": ["mechine learning","clustering"],
        "tags": ["math","mechine learning"],
        "url": "http://localhost:4000/mechine%20learning/clustering/kmeans/"
      },{
        "title": "股票作手回忆录",
        "excerpt":"《漫步华尔街(原书第11版)》 Author: 伯顿G.马尔基尔Publisher: 机械工业出版社Publish Date: 2018-1 过度自信-每个人都认为自己的投资水平超过80%的人 判断偏差-被感觉，错觉误导 羊群效应-从众心里 损失厌恶-不愿意接受确定的损失，宁可冒更大的损失风险放手一搏 Comment:行为金融学 Page 224 · 2018-10-10 收益率=无风险收益率+B（市场收益率-无风险收益率） 实际收益率大于经过B算出来来的收益率，则称实现了A收益率 Comment:alpha 和beta收益 2018-10-10 规则1. 只买入盈利增长预期连续5年以上超过平均水平的公司 规则2.千万不能为一只股票付出超过其坚实基础价值的价格 规则3.寻找投资者可在其预期增长故事之上建立空中楼阁的股票 Comment:技术分析和基础面结合 Page 114 · 2018-10-09 从股票的四条估值规则中，我们可以看出：公司的增长率越高、增长持续期越长，其股票的坚实基础价值（及市盈率）便越高；公司的股利发放越多，其股票的坚实基础价值（及市盈率）便越高；公司股票的风险越低，其坚实基础价值（及市盈率）便越高；一般利率水平越低，股票的坚实基础价值（及市盈率）便越高。 Page 107 · 2018-10-09 《证券分析》一书的作者本杰明·格雷厄姆洞若观火的见解令我折服，他在书中写到：归根结底，股票市场不是投票机，而是称重机。估值标准并未改变，最终，任何股票的价值只能等于该股票能给投资者带来的现金流的现值。归根结底，真实价值终会胜出。 Comment:价值的真实含义 Page 85 · 2018-10-09 其实，要在股市赚钱并不难。我们在后面的内容中会看到，投资者只要购买并持有涵盖范围广泛的股票组合，就能获得相当不错的长期回报。真正难以避免的，是受到诱惑时情不自禁地将自己的资金投向短期快速致富的投机盛宴之中。 Page 77 · 2018-10-09 投资的关键不在于某个行业会给社会带来多大影响，甚至也不在于该行业本身会有多大增长，而在于该行业是否能够创造利润并持续盈利。历史告诉我们，所有过度繁荣的市场最终均将屈服于引力定律。 Comment:投资重点 Page 76 · 2018-10-09 1955～1990年，日本的房地产价格上涨了75倍以上。据估计，截至1990年，日本所有房地产的总值接近20万亿美元，相当于20%多的全球财富，或相当于全球股市总市值的2倍。 Comment:日本的房地产和股市泡沫 Page 54 · 2018-10-09 的确，过去的投资者利用IPO建造了很多空中楼阁。请记住IPO新股的主要抛售者正是公司的管理层。他们力图把握时机，或随着公司发展达到高峰，或当投资者热情高涨地追逐某个流行热点时，抛售自己持有的本公司股票。在这样的情况下，对投资者来说，追赶潮流的急切心情，哪怕是追逐高成长行业的股票，只会带来无利可图的忙忙碌碌。 Comment:IPO Page 54 · 2018-10-09 投资者对待新股应持有一定的怀疑态度，这才是健康的投资心态。 Page 54 · 2018-10-09 ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/%E8%82%A1%E7%A5%A8%E4%BD%9C%E6%89%8B%E5%9B%9E%E5%BF%86%E5%BD%95/"
      },{
        "title": "财富自由之路",
        "excerpt":"fddfd    Author: 李笑来  Publisher: 电子工业出版社  Publish Date: 2017-10-1我送过你一把”钥匙，再送你一把刀—这本书是否“价值连城就看你的了。为什么？！凭什么？！为什么到最后我没有责任，做不好反倒要怪你呢？—好问题！   Comment:钥匙🔑是不盯着锁上找的钥匙，解决生气的方法是去逗她笑，不是盯着问题。刀🔪是一次只做一件事的刀 2018-10-25 你的投资依据必须靠且仅靠你自己的深入思考而得到。 2018-10-25 在投资领域，不要急于行动 2018-10-25 这种思考的结果。通常被含混地措述为“有修养”。对这个世界越清楚的描述就越有指导意义，并能让我们有所依据地作出判断：而那些含混的描述常常让我们“不知其所以然”。有了这种清楚的描述之后，我们就很容易显得“有修养”了，因为我们限很清楚：我们的“需求”是我们的，别人的“需求是别人的，我们的“需求”和别人的“需求”不仅不一定相同，也常常没必要相同。于是，我们段没必要把时间耗费在这种“必然的不同”上一随它去吧 2018-10-25 你不断成长的结果，就是你终将被低估—这是必然的。 2018-10-25 所谓“个人财富自由”，是指某个人再也不用为了满足生活必需而出售自己的时间了。 2018-10-25 当你专注的时候，时间会飞速流逝 2018-10-25 你知道你为什么要对自己的美好未来深信不疑吗？我甚至常常向身边的人一遍一遍地灌输“你必须盲目笃信”的观念—对你认定的这件事的笃信要盲目到谁都不能动摇的地步…因为啊。因为这件事除了你自己之外没有人会相信！ 2018-10-25 ","categories": ["invest"],
        "tags": ["李笑来"],
        "url": "http://localhost:4000/invest/%E8%B4%A2%E5%AF%8C%E8%87%AA%E7%94%B1%E4%B9%8B%E8%B7%AF/"
      },{
        "title": "少年",
        "excerpt":"你曾是少年\t\t\t\t\t\t\tYour browser does not support the audio element.\t\t\t你曾是少年，倔强勇敢不改变；你曾是少年，拌嘴吐槽也争先；你曾是少年，啤酒饺子鸡蛋面；你曾是少年，熬夜看书少睡眠；你曾是少年，周易老庄百家言；你曾是少年，江天万里不挂念；你曾是少年，物化生数都等闲；你曾是少年，追云逐日不自谦；你曾是少年，锦绣河山誓踏遍；你曾是少年，怒马扬尘花看厌；你曾是少年，转身告别不留恋。乘风破浪斩敌前，男儿至死是少年。  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/recipes/shao-nian/"
      }]
