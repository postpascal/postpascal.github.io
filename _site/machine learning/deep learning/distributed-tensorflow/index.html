<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.1.0
    Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    
    <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Distributed Tensorflow | 紫禁之巅</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Distributed Tensorflow" />
<meta name="author" content="Zhang Keke" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Table of Contents" />
<meta property="og:description" content="Table of Contents" />
<link rel="canonical" href="http://localhost:4000/machine%20learning/deep%20learning/distributed-tensorflow/" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/deep%20learning/distributed-tensorflow/" />
<meta property="og:site_name" content="紫禁之巅" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-02T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Table of Contents","author":{"@type":"Person","name":"Zhang Keke"},"@type":"BlogPosting","url":"http://localhost:4000/machine%20learning/deep%20learning/distributed-tensorflow/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/images/tree1.svg"},"name":"Zhang Keke"},"headline":"Distributed Tensorflow","dateModified":"2018-04-02T00:00:00+08:00","datePublished":"2018-04-02T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/deep%20learning/distributed-tensorflow/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="紫禁之巅" href="/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  distributed-tensorflow">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/">Home</a></li><li><a href="/posts/">Posts</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="/recipes/">Recipes</a></li><li><a href="/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
      <a href="/" class="site-logo" rel="home" title="紫禁之巅">
        <img src="/images/tree1.svg" class="site-logo-img animated fadeInDown" alt="紫禁之巅">
      </a>
    
    
      <h1 class="site-title animated fadeIn"><a href="/">紫禁之巅</a></h1>
      <p class="site-description animated fadeIn" itemprop="description">stay hungry stay foolish</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">Distributed Tensorflow
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/images/t.svg" class="author-avatar u-photo" alt="Zhang Keke"><div class="author-info"><div class="author-name">
        <em>by</em> <span class="p-name">Zhang Keke</span>
      </div><ul class="author-links"><li class="author-link">
            <a class="u-url" rel="me" href="hhttps://500px.com/apostpascal"><i class="fab fa-500px fa-lg" title="500px"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://github.com/postpascal"><i class="fab fa-github-square fa-lg" title="GitHub"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://weibo.com/u/2858024844?refer_flag=1001030101_"><i class="fab fa-weibo fa-lg" title="Weibo"></i></a>
          </li></ul>

<span class="read-time">3 min read</span>

    <time class="page-date dt-published" datetime="2018-04-02T00:00:00+08:00"><a class="u-url" href="">April 2, 2018</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/categories/#machine-learning" title="Pages filed under Machine Learning">Machine Learning</a></li><li class="page-taxonomy"><a class="p-category" href="/categories/#deep-learning" title="Pages filed under Deep Learning">Deep Learning</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/tags/#tensorflow" title="Pages tagged Tensorflow" rel="tag">Tensorflow</a></li><li class="page-taxonomy"><a href="/tags/#distribution" title="Pages tagged Distribution" rel="tag">Distribution</a></li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <div id="entry-table-of-contents" class="toc-wrapper">
  <h2 id="toc-toggle" class="no_toc">
  Table of Contents <i class="toc-toggle-icon fas fa-chevron-down"></i>
</h2>
<ol id="markdown-toc">
  <li><a href="#terminology" id="markdown-toc-terminology">Terminology</a></li>
  <li><a href="#multiple-devices-on-a-single-machine" id="markdown-toc-multiple-devices-on-a-single-machine">Multiple Devices on a Single Machine</a>    <ol>
      <li><a href="#dependencies" id="markdown-toc-dependencies">Dependencies:</a></li>
      <li><a href="#managing-gpu-ram" id="markdown-toc-managing-gpu-ram">Managing GPU RAM</a></li>
      <li><a href="#placing-operations-on-devices" id="markdown-toc-placing-operations-on-devices">Placing operations on Devices</a></li>
    </ol>
  </li>
  <li><a href="#multiple-devices-across-multiple-servers" id="markdown-toc-multiple-devices-across-multiple-servers">Multiple Devices Across Multiple Servers</a></li>
  <li><a href="#advantages" id="markdown-toc-advantages">Advantages:</a></li>
  <li><a href="#parallelizing-neural-network-on-a--tensorflow-cluster" id="markdown-toc-parallelizing-neural-network-on-a--tensorflow-cluster">Parallelizing Neural Network on a  Tensorflow Cluster</a>    <ol>
      <li><a href="#one-neural-network-per-device" id="markdown-toc-one-neural-network-per-device">One Neural Network per device</a></li>
      <li><a href="#in-graph-versus-between-graph-replication-ensemble" id="markdown-toc-in-graph-versus-between-graph-replication-ensemble">In-Graph Versus Between-Graph Replication (ensemble)</a></li>
    </ol>
  </li>
  <li><a href="#model-parallelism" id="markdown-toc-model-parallelism">Model Parallelism</a></li>
  <li><a href="#data-parallelism" id="markdown-toc-data-parallelism">Data Parallelism</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference:</a></li>
</ol>

</div>

<h3 id="terminology">Terminology</h3>
<ul>
  <li>Cluster: A cluster is composed of one or more Tensorflow servers, called tasks.</li>
  <li>Job: a group of tasks that have a common roal. For instance, parameters server, worker server.</li>
  <li>Task: A task corresponds to a specific TensorFlow server, and typically corresponds to a single process. A task belongs to a particular “job” and is identified by its index within that job’s list of tasks.</li>
</ul>

<hr />

<h3 id="multiple-devices-on-a-single-machine">Multiple Devices on a Single Machine</h3>
<h4 id="dependencies">Dependencies:</h4>
<ul>
  <li>CUDA: Nvidia’s Compute Unified Device Architecture Library allows developers to call GPUs</li>
  <li>cuDNN: CUDA deep neural network library to call CUDA</li>
  <li>tensorflow-gpu</li>
</ul>

<figure style="width: 400px" class="align-center">
  <img src="http://localhost:4000/images/resources/EF7FF3FE23EE675AA38920A81F91B7E7.png" alt="" />
  <figcaption>CUDA, cuDNN and Tensorflow </figcaption>
</figure>

<h4 id="managing-gpu-ram">Managing GPU RAM</h4>
<p>By default Tensorflow will grab all the RAM in all available GPUs the first time you run a graph, so if you run a second Tensorflow program, Memory error showing up.</p>

<ul>
  <li>One solution is to run each process  on different GPU card.</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># run script in specific GPU card</span>
CUDA_VISIBLE_DEVICES <span class="o">=</span> 0,1 python3 program.py
</code></pre></div></div>
<ul>
  <li>Another option is to set a threshold for Tensorflow, which limits the program can only grab a fraction of GPUs’ memory.</li>
</ul>

<pre><code class="language-python3">config = tf.ConfigProto()
config.gpu_option.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config)
</code></pre>
<h4 id="placing-operations-on-devices">Placing operations on Devices</h4>

<p>Using the following code to place operation to a specific device, otherwise it will place to the default device.</p>
<blockquote>
  <p>Cautions: there is no way to pin nodes on a specific CPUs or a subset CPUS</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"/cpu:0"</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="multiple-devices-across-multiple-servers">Multiple Devices Across Multiple Servers</h3>
<ul>
  <li>Create a Clusesr</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cluser_spec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ClusterSpec</span><span class="p">({</span>
    <span class="s">"worker"</span><span class="p">:</span> <span class="p">[</span>
        <span class="s">"machine-a.example.com:2222"</span><span class="p">,</span> <span class="c">#/job:worker/task:0</span>
        <span class="s">"machine-b.example.com:2222"</span> <span class="c">#/job:worker/task:1</span>
    <span class="p">],</span>
    <span class="s">"ps"</span><span class="p">:</span> <span class="p">[</span>
        <span class="s">"machine-a.example.com:2221"</span><span class="p">,</span> <span class="c">#/job:ps/task:0</span>
    <span class="p">]})</span>
</code></pre></div></div>

<figure style="width: 400px" class="align-center">
  <img src="http://localhost:4000/images/resources/5246338809A70A300174F06780A8F7A6.png" alt="" />
  <figcaption>Distributed Tensorflow </figcaption>
</figure>

<ul>
  <li>Run a server</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">server</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Server</span><span class="p">(</span><span class="n">cluster_spec</span><span class="p">,</span> <span class="n">job_name</span><span class="o">=</span><span class="s">"worker"</span><span class="p">,</span><span class="n">task_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c"># start first worker task</span>
</code></pre></div></div>

<ul>
  <li>Use <code class="highlighter-rouge">server.join()</code> to wait servers finish</li>
</ul>

<blockquote>
  <p>In distribution tensorflow, variables maintain by resource container. In other words, we can access the variable across different sessions and servers. To avoid name clash, we can wrap variables by using <code class="highlighter-rouge">tf.variable_scope</code> or <code class="highlighter-rouge">tf.container</code>. <code class="highlighter-rouge">tf.container</code> is easy to reset and release variables.</p>
  <h3 id="advantages">Advantages:</h3>
  <ul>
    <li>Be able to explore a much larger hyperparameter space when fine-tuning your model</li>
    <li>Large ensembles of NN efficiently</li>
  </ul>
</blockquote>

<h3 id="parallelizing-neural-network-on-a--tensorflow-cluster">Parallelizing Neural Network on a  Tensorflow Cluster</h3>
<h4 id="one-neural-network-per-device">One Neural Network per device</h4>

<figure style="width: 400px" class="align-center">
  <img src="http://localhost:4000/images/resources/E856AD8B0ED2C7CDCDB45564C7422896.png" alt="" />
  <figcaption> </figcaption>
</figure>

<p>One client per neural network per device.Each device running similar neural network with different hyperparameters.This solution is perfect for hyperparameter tuning.</p>

<h4 id="in-graph-versus-between-graph-replication-ensemble">In-Graph Versus Between-Graph Replication (ensemble)</h4>
<ul>
  <li>In-Graph: one client,one graph, one session maintains all stuff, one neural network per device.</li>
</ul>

<figure style="width: 400px" class="align-center">
  <img src="http://localhost:4000/images/resources/2DA0DD8414DA59C6A9C4DA0C4AC202BE.png" alt="" />
  <figcaption>In-Graph </figcaption>
</figure>

<ul>
  <li>Between-Graph: several clients maintain <code class="highlighter-rouge">Input</code>, <code class="highlighter-rouge">Output</code> and <code class="highlighter-rouge">Neural Network</code>. Each Neural network is an individual graph.</li>
</ul>

<figure style="width: 400px" class="align-center">
  <img src="http://localhost:4000/images/resources/A15BD70A71FED16E09A3B9128E767E5D.png" alt="" />
  <figcaption>Between-Graph</figcaption>
</figure>

<h3 id="model-parallelism">Model Parallelism</h3>
<ul>
  <li>Separate Neural network Horizontally or Vertically.</li>
</ul>

<figure style="width: 400px" class="align-center">
  <img src="http://localhost:4000/images/resources/69478BB7441E6A92A5E495EA467E9BC0.png" alt="" />
  <figcaption>Model Parallelism</figcaption>
</figure>
<h3 id="data-parallelism">Data Parallelism</h3>
<ul>
  <li>Each device running same neural network, but feeded different mini-batch data. For each iteration, neural network fetch parameters from <code class="highlighter-rouge">PS</code>, then Gradients from each network aggrated to update parameters in <code class="highlighter-rouge">PS</code>. There are two main approach, <em>synchronous</em> and <em>asynchronous</em>.</li>
  <li>With synchronous updates, the aggregator waits for all gradients to be available before computing the average and applying the result.The downside is faster replica have to wait slower one at every iteration. To make it more efficient, we can update parameters only if a fraction of replicas has finished.</li>
  <li>With asynchronous updates, whenever a replica has finished computing the gradients, it immediately uses them to update the model parameters.</li>
</ul>

<figure style="width: 400px" class="align-center">
  <img src="http://localhost:4000/images/resources/858A8FB5C9CCA2CD3E954C58C610187A.png" alt="" />
  <figcaption>Data Parallelism</figcaption>

</figure>
<hr />
<h3 id="reference">Reference:</h3>
<ul>
  <li><a href="https://www.tensorflow.org/deploy/distributed">Distributed Tensorflow</a>: Tensorflow official doc</li>
  <li><a href="https://github.com/tensorflow/ecosystem">Echosystem</a>: tensorflow official github project, a integration of tensorflow with other open-source framework</li>
  <li><a href="https://g.co/kgs/dHRoaa">Hands-on Machine Learning with Scikit_Learn &amp; Tensorflow</a>: Chapter 12, Distributing Tensorflow Across Devices and Server</li>
</ul>

<hr />

        </div>

        
          <div class="page-share">
  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fdeep%2520learning%2Fdistributed-tensorflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--facebook btn--small"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i> <span>Share</span></a>
  <a href="https://twitter.com/intent/tweet?text=Distributed+Tensorflow%20http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fdeep%2520learning%2Fdistributed-tensorflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--twitter btn--small"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> <span>Tweet</span></a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fdeep%2520learning%2Fdistributed-tensorflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--linkedin btn--small"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> <span>LinkedIn</span></a>
  <a href="https://reddit.com/submit?title=Distributed+Tensorflow&url=http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fdeep%2520learning%2Fdistributed-tensorflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--reddit btn--small"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i> <span>Reddit</span></a>
</div>

        

        
          

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/machine%20learning/reading/recap/Machine-Learning-tips/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Machine Learning Tips

      </span>
    </a>
  

  
    <a class="page-next" href="/math/products/">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Products
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a></div><div class="copyright">
    
      <p>&copy; 2018 紫禁之巅. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.7/js/all.js"></script>


<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  </body>

</html>
