<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.1.0
    Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    
    <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>PCA | 紫禁之巅</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="PCA" />
<meta name="author" content="Zhang Keke" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Blog Source:" />
<meta property="og:description" content="Blog Source:" />
<link rel="canonical" href="http://localhost:4000/machine%20learning/pca/" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/pca/" />
<meta property="og:site_name" content="紫禁之巅" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-09-11T20:08:50+08:00" />
<script type="application/ld+json">
{"description":"Blog Source:","author":{"@type":"Person","name":"Zhang Keke"},"@type":"BlogPosting","url":"http://localhost:4000/machine%20learning/pca/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/images/tree1.svg"},"name":"Zhang Keke"},"headline":"PCA","dateModified":"2017-09-11T20:08:50+08:00","datePublished":"2017-09-11T20:08:50+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/pca/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="紫禁之巅" href="/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  pca">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/">Home</a></li><li><a href="/posts/">Posts</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="/recipes/">Recipes</a></li><li><a href="/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
      <a href="/" class="site-logo" rel="home" title="紫禁之巅">
        <img src="/images/tree1.svg" class="site-logo-img animated fadeInDown" alt="紫禁之巅">
      </a>
    
    
      <h1 class="site-title animated fadeIn"><a href="/">紫禁之巅</a></h1>
      <p class="site-description animated fadeIn" itemprop="description">stay hungry stay foolish</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">PCA
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/images/t.svg" class="author-avatar u-photo" alt="Zhang Keke"><div class="author-info"><div class="author-name">
        <em>by</em> <span class="p-name">Zhang Keke</span>
      </div><ul class="author-links"><li class="author-link">
            <a class="u-url" rel="me" href="hhttps://500px.com/apostpascal"><i class="fab fa-500px fa-lg" title="500px"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://github.com/postpascal"><i class="fab fa-github-square fa-lg" title="GitHub"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://weibo.com/u/2858024844?refer_flag=1001030101_"><i class="fab fa-weibo fa-lg" title="Weibo"></i></a>
          </li></ul>

<span class="read-time">~1 min read</span>

    <time class="page-date dt-published" datetime="2017-09-11T20:08:50+08:00"><a class="u-url" href="">September 11, 2017</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/categories/#machine-learning" title="Pages filed under machine learning">machine learning</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/tags/#math" title="Pages tagged math" rel="tag">math</a></li><li class="page-taxonomy"><a href="/tags/#pca" title="Pages tagged PCA" rel="tag">PCA</a></li><li class="page-taxonomy"><a href="/tags/#dimension-reduction" title="Pages tagged dimension reduction" rel="tag">dimension reduction</a></li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <p><a href="http://blog.codinglabs.org/articles/pca-tutorial.html">Blog Source:</a></p>

<h2 id="方差">方差</h2>
<p>上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：</p>

<script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum_{i=1}^m{(a_i-\mu)^2}</script>

<p>由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>

<script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum_{i=1}^m{a_i^2}</script>

<p>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p>
<h2 id="协方差">协方差</h2>
<p>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。</p>

<p>如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。</p>

<p>数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：</p>

<script type="math/tex; mode=display">Cov(a,b)=\frac{1}{m}\sum_{i=1}^m{a_ib_i}</script>

<p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。</p>

<p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p>

<p>至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。</p>

<h2 id="协方差矩阵">协方差矩阵</h2>
<p>上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。</p>

<p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：
假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：</p>

<script type="math/tex; mode=display">% <![CDATA[
X= \begin{pmatrix} a_1 & a_2 & \cdots & a_m \\ b_1 & b_2 & \cdots & b_m \end{pmatrix} %]]></script>

<p>然后我们用X乘以X的转置，并乘上系数$1/m$：</p>

<script type="math/tex; mode=display">% <![CDATA[
\frac{1}{m}XX^\mathsf{T}=\begin{pmatrix} \frac{1}{m}\sum_{i=1}^m{a_i^2} & \frac{1}{m}\sum_{i=1}^m{a_ib_i} \\ \frac{1}{m}\sum_{i=1}^m{a_ib_i} & \frac{1}{m}\sum_{i=1}^m{b_i^2} \end{pmatrix} %]]></script>

<p>奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p>

<p>根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：</p>

<p>设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设<script type="math/tex">C=\frac{1}{m}XX^\mathsf{T}</script>则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。</p>

<p>##协方差矩阵对角化
根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p>

<p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{l l l} D & = & \frac{1}{m}YY^\mathsf{T} \\ & = & \frac{1}{m}(PX)(PX)^\mathsf{T} \\ & = & \frac{1}{m}PXX^\mathsf{T}P^\mathsf{T} \\ & = & P(\frac{1}{m}XX^\mathsf{T})P^\mathsf{T} \\ & = & PCP^\mathsf{T} \end{array} %]]></script>

<p>现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足<script type="math/tex">PCP^\mathsf{T}</script>是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p>

<p>至此，我们离“发明”PCA还有仅一步之遥！</p>

<p>现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。</p>

<p>由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p>

<p>1）实对称矩阵不同特征值对应的特征向量必然正交。</p>

<p>2）设特征向量<script type="math/tex">\lambda</script>重数为r，则必然存在r个线性无关的特征向量对应于\lambda，因此可以将这r个特征向量单位正交化。</p>

<p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为<script type="math/tex">e_1,e_2,\cdots,e_n</script>，我们将其按列组成矩阵：</p>

<script type="math/tex; mode=display">% <![CDATA[
E=\begin{pmatrix} e_1 & e_2 & \cdots & e_n \end{pmatrix} %]]></script>

<p>则对协方差矩阵C有如下结论：</p>

<script type="math/tex; mode=display">% <![CDATA[
E^\mathsf{T}CE=\Lambda=\begin{pmatrix} \lambda_1 & & & \\ & \lambda_2 & & \\ & & \ddots & \\ & & & \lambda_n \end{pmatrix} %]]></script>

<p>其中<script type="math/tex">\Lambda</script>为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。</p>

<p>以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。</p>

<p>到这里，我们发现我们已经找到了需要的矩阵P：</p>

<script type="math/tex; mode=display">P=E^\mathsf{T}</script>

<p>P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照<script type="math/tex">\Lambda</script>中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p>

<p>至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。</p>

<h1 id="算法及实例">算法及实例</h1>
<p>为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。</p>

<h2 id="pca算法">PCA算法</h2>
<p>总结一下PCA的算法步骤：</p>

<p>设有m条n维数据。</p>

<p>1）将原始数据按列组成n行m列矩阵X</p>

<p>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p>

<p>3）求出协方差矩阵<script type="math/tex">C=\frac{1}{m}XX^\mathsf{T}</script></p>

<p>4）求出协方差矩阵的特征值及对应的特征向量</p>

<p>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</p>

<p>6）Y=PX即为降维到k维后的数据</p>

<h2 id="实例">实例</h2>
<p>这里以上文提到的</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{pmatrix} -1 & -1 & 0 & 2 & 0 \\ -2 & 0 & 0 & 1 & 1 \end{pmatrix} %]]></script>

<p>为例，我们用PCA方法将这组二维数据其降到一维。</p>

<p>因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：</p>

<script type="math/tex; mode=display">% <![CDATA[
C=\frac{1}{5}\begin{pmatrix} -1 & -1 & 0 & 2 & 0 \\ -2 & 0 & 0 & 1 & 1 \end{pmatrix}\begin{pmatrix} -1 & -2 \\ -1 & 0 \\ 0 & 0 \\ 2 & 1 \\ 0 & 1 \end{pmatrix}=\begin{pmatrix} \frac{6}{5} & \frac{4}{5} \\ \frac{4}{5} & \frac{6}{5} \end{pmatrix} %]]></script>

<p>然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为：</p>

<script type="math/tex; mode=display">\lambda_1=2,\lambda_2=2/5</script>

<p>其对应的特征向量分别是：</p>

<script type="math/tex; mode=display">c_1\begin{pmatrix} 1 \\ 1 \end{pmatrix},c_2\begin{pmatrix} -1 \\ 1 \end{pmatrix}</script>

<p>其中对应的特征向量分别是一个通解，<script type="math/tex">c_1和c_2</script>可取任意实数。那么标准化后的特征向量为：</p>

<script type="math/tex; mode=display">\begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix},\begin{pmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}</script>

<p>因此我们的矩阵P是：</p>

<script type="math/tex; mode=display">% <![CDATA[
P=\begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ -1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix} %]]></script>

<p>可以验证协方差矩阵C的对角化：</p>

<script type="math/tex; mode=display">% <![CDATA[
PCP^\mathsf{T}=\begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ -1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}\begin{pmatrix} 6/5 & 4/5 \\ 4/5 & 6/5 \end{pmatrix}\begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{2} \\ 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}=\begin{pmatrix} 2 & 0 \\ 0 & 2/5 \end{pmatrix} %]]></script>

<p>最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：</p>

<script type="math/tex; mode=display">% <![CDATA[
Y=\begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}\begin{pmatrix} -1 & -1 & 0 & 2 & 0 \\ -2 & 0 & 0 & 1 & 1 \end{pmatrix}=\begin{pmatrix} -3/\sqrt{2} & -1/\sqrt{2} & 0 & 3/\sqrt{2} & -1/\sqrt{2} \end{pmatrix} %]]></script>

<p>降维投影结果如下图:</p>

<figure style="width: 250px" class="align-center">
  <img src="http://localhost:4000/images/pca.png" alt="" />
  <figcaption>PCA image.</figcaption>
</figure>

<h2 id="进一步讨论">进一步讨论</h2>
<p>根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p>

<p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p>

<p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p>

<p>希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。</p>

        </div>

        
          <div class="page-share">
  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fpca%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--facebook btn--small"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i> <span>Share</span></a>
  <a href="https://twitter.com/intent/tweet?text=PCA%20http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fpca%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--twitter btn--small"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> <span>Tweet</span></a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fpca%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--linkedin btn--small"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> <span>LinkedIn</span></a>
  <a href="https://reddit.com/submit?title=PCA&url=http%3A%2F%2Flocalhost%3A4000%2Fmachine%2520learning%2Fpca%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--reddit btn--small"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i> <span>Reddit</span></a>
</div>

        

        
          

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/math/mle/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Maximum Likelihood Estimation

      </span>
    </a>
  

  
    <a class="page-next" href="/extraction/reading/%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3/">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        智能时代
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a></div><div class="copyright">
    
      <p>&copy; 2018 紫禁之巅. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.7/js/all.js"></script>


<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  </body>

</html>
